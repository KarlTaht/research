# Transformer model matching original grokking paper setup
# Decoder-only transformer with causal attention
# Input: [a, op, b, =] token sequence -> predict result

# Data
p: 113
operation: "add"
train_frac: 0.3
data_seed: 42

# Model (matching original paper)
model_type: "transformer"
hidden_dim: 128  # d_model
n_layers: 2      # 2 transformer blocks
n_heads: 4       # 4 attention heads (32 dims per head)

# Training
epochs: 100000
lr: 0.001
weight_decay: 1.0  # Higher weight decay for grokking
optimizer: "adamw"
grad_clip: 1.0     # Gradient clipping for stable training

# No routing regularization for transformer
routing_regularizer: null
lambda_routing: 0.0
lambda_spectral: 0.0

# Logging
log_every: 100
save_routing_every: 1000
checkpoint_every: 5000

# Experiment
name: "transformer_113_add"
seed: 42
