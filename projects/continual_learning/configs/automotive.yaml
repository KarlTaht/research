# Training config for automotive corpus
# Target: ~25-30M parameters, 1024 context length

experiment_name: "automotive_baseline"

model:
  d_model: 512
  n_layers: 6
  n_heads: 8        # d_head = 64
  d_ff: 2048        # 4x d_model
  max_seq_len: 1024
  dropout: 0.1
  dtype: bfloat16

data:
  corpus_dir: "data/corpus_automotive"
  tokenizer: "combined_bpe_32768"
  max_length: 1024
  # subset_size: 10000  # Uncomment for quick tests

training:
  batch_size: 16
  learning_rate: 3e-4
  min_learning_rate: 3e-5
  lr_decay: cosine
  warmup_ratio: 0.05
  weight_decay: 0.01
  num_epochs: 3

  log_every: 100
  eval_every: 500
  save_every: 1

  max_grad_norm: 1.0

evaluation:
  generation_prompts:
    - "The engine"
    - "When you drive"
    - "The best way to"
  max_generation_length: 100
  temperature: 0.8
  top_k: 50
