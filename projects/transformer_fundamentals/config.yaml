# Configuration for ReferenceTransformer training on SAMSum

model:
  vocab_size: 50257  # GPT-2 tokenizer vocab size
  d_model: 256        # Model dimension (reduced for faster training)
  n_heads: 8          # Number of attention heads
  n_encoder_layers: 4 # Encoder depth (reduced from paper's 6)
  n_decoder_layers: 4 # Decoder depth (reduced from paper's 6)
  d_ff: 1024          # Feed-forward hidden dimension (4 * d_model)
  dropout: 0.1        # Dropout probability
  max_seq_len: 512    # Maximum sequence length

data:
  dataset_name: samsum
  max_dialogue_length: 256  # Max tokens for dialogue (source)
  max_summary_length: 64    # Max tokens for summary (target)
  use_subset: false         # SAMSum is already small, use full dataset
  # subset_size: 1000       # Uncomment for quick testing

training:
  batch_size: 16          # Batch size (adjust based on GPU memory)
  learning_rate: 0.0001   # Learning rate (1e-4)
  num_epochs: 10          # Number of training epochs
  gradient_clip: 1.0      # Gradient clipping max norm
  save_every: 2           # Save checkpoint every N epochs
  device: cuda            # Device: 'cuda' or 'cpu'
  seed: 42                # Random seed for reproducibility

# Experiment tracking
experiment:
  name: transformer_samsum_baseline
  description: Baseline validation of ReferenceTransformer on SAMSum dialogue summarization
