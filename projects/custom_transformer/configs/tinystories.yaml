# TinyStories training configuration
experiment_name: "custom_transformer_tinystories"

model:
  d_model: 256
  n_blocks: 4
  n_heads: 4
  d_ffn: 512
  max_seq_len: 256

data:
  dataset: "tinystories"  # Uses dataset registry
  max_length: 256
  use_subset: true
  subset_size: 50000
  val_subset_size: 1000

training:
  batch_size: 16
  learning_rate: 0.0003
  num_epochs: 5

  # Logging intervals
  log_every: 100         # Log to training logger every N steps
  eval_every: 500        # Run validation every N steps
  save_every: 1          # Save checkpoint every N epochs

  # Stability
  max_grad_norm: 1.0     # Gradient clipping
  max_nan_count: 10      # Halt training after N NaN occurrences

evaluation:
  generation_prompts:
    - "Once upon a time"
    - "The little girl"
    - "One day, a boy named"
    - "In a small village"
  max_generation_length: 100
  temperature: 0.8
  top_k: 50
  evaluate_coherence: true  # Use Claude Haiku if ANTHROPIC_API_KEY is set
