# TinyStories training with custom BPE tokenizer (4096 vocab)
#
# This config uses a dataset-specific tokenizer trained on TinyStories,
# which provides 92% smaller embedding matrices compared to GPT-2:
#   - GPT-2: vocab_size=50,257 -> ~12.9M embedding params (at d_model=256)
#   - Custom: vocab_size=4,096 -> ~1.0M embedding params (at d_model=256)
#
# The custom tokenizer achieves 99%+ coverage on TinyStories while being
# much more efficient for small-scale experiments.

experiment_name: "custom_transformer_tinystories_bpe4096"

model:
  d_model: 256
  n_blocks: 4
  n_heads: 4
  d_ffn: 512
  max_seq_len: 256

data:
  dataset: "tinystories"  # Uses dataset registry
  tokenizer: "tinystories_bpe_4096"  # Custom tokenizer in assets/models/tokenizers/
  max_length: 256
  use_subset: true
  subset_size: 50000
  val_subset_size: 1000

training:
  batch_size: 16
  learning_rate: 0.0003
  num_epochs: 5

  # Logging intervals
  log_every: 100         # Log to training logger every N steps
  eval_every: 500        # Run validation every N steps
  save_every: 1          # Save checkpoint every N epochs

  # Stability
  max_grad_norm: 1.0     # Gradient clipping
  max_nan_count: 10      # Halt training after N NaN occurrences

evaluation:
  generation_prompts:
    - "Once upon a time"
    - "The little girl"
    - "One day, a boy named"
    - "In a small village"
  max_generation_length: 100
  temperature: 0.8
  top_k: 50
  evaluate_coherence: true  # Use Claude Haiku if ANTHROPIC_API_KEY is set
